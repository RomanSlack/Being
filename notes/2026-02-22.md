# 2026-02-22 — First GaussianTalker Training Run

## Summary

First end-to-end test of GaussianTalker on the roman dataset. Switched from InsTaG to GaussianTalker based on research (see RESEARCH.md). Goal: prove the pipeline works, not achieve final quality.

## Setup

- **Hardware**: A100 80GB on RunPod
- **Input data**: 10 seconds of video (250 frames at 25fps), reused from InsTaG preprocessing
- **Audio features**: wav2vec/esperanto 44-dim (copied as aud_ds.npy, changed audio_in_dim from 29→44)
- **Face model**: BFM 2009 (same as InsTaG), added vertices field to track_params.pt
- **Config**: `64_dim_1_transformer.py` — 64-dim model, 1 transformer layer, 32 batch size
- **AU data**: Dummy (no real OpenFace extraction yet)

## Data Compatibility

InsTaG preprocessed data was ~95% reusable. Only two fixes needed:
1. Added `vertices` key to `track_params.pt` (reconstructed from BFM coefficients)
2. Copied `aud_eo.npy` → `aud_ds.npy` and changed `audio_in_dim` from 29 to 44 in `scene/deformation.py`

## Installation Issues & Fixes

| Issue | Fix |
|-------|-----|
| Missing `open3d` | `pip install open3d` (needed `--ignore-installed blinker` first) |
| Missing `tkinter` | `apt-get install python3-tk` |
| Missing `cv2` | `pip install opencv-python-headless` |
| Missing `wandb` | `pip install wandb` + `WANDB_MODE=disabled` |
| `mmcv` too heavy / build timeout | Replaced with `mmengine` — patched `train.py` and `render.py` imports |
| `imageio` missing for render | `pip install imageio imageio-ffmpeg` |
| `ffmpeg` missing on pod | `apt-get install ffmpeg` |
| DataLoader crash (`num_workers=32`) | Reduced to `num_workers=4` in train.py (3 locations) |
| `render.py` hardcodes `process_until=1000` | Patched to `min(1000, len(viewpoint_stack.dataset))` for our 250-frame dataset |

## Training

### Coarse Stage (7999 iterations)
- **Time**: ~5 minutes
- **Speed**: ~28 it/s
- **Final PSNR**: ~34
- **Gaussians**: 32,655

### Fine Stage (stopped early at iter 2000/10000)
- **Speed**: ~1.0-1.1 s/iter
- **Stopped at**: iter 2000 (20%), PSNR 35.37
- **Reason for stopping**: Quality plateaued for what 10s of data can provide. The remaining issues (missing hair, no mouth interior) need better input data, not more iterations. Diminishing returns from here — save GPU hours.

Fine stage enables `lip_fine_tuning` and `depth_fine_tuning` for better mouth and depth quality.

## Render Quality Snapshots

### Coarse stage (iter 7500) — ghostly
- Face is transparent/ghostly, heavy artifacts
- General shape is there but not usable
- This is expected — coarse stage only learns rough geometry

### Fine stage (iter 500) — already solid
- Face becomes solid and recognizable
- Skin color and texture reasonable
- Massive jump from coarse → fine

### Fine stage (iter 1500) — improving
- Sharper features, more natural lighting
- Less waxy appearance
- Clear improvement over iter 500

### Fine stage (iter 2000) — final checkpoint
- Slightly sharper than iter 1500, more natural skin shading
- Quality gains diminishing — bottleneck is input data, not training iterations
- Decided to stop training here

### Known Issues (at iter 2000)
1. **Missing hair / top of head**: BFM mesh only covers the face (34,650 vertices), not scalp/hair. Hair must be learned via Gaussian densification from training images. With only 10s of tightly-cropped video, there's not enough signal for the hair region.
2. **No mouth interior**: BFM has no teeth/tongue geometry. Model must learn mouth interior purely from pixels. With limited mouth-open frames in 10s of video and dummy AU data, this is undertrained.
3. Both issues are solvable with longer input video (3-5 min) and real preprocessing (OpenFace AU, teeth masks).

## Inference FPS Benchmark

**Result: 2.4 FPS — NOT real-time.** The paper's "130fps" claim is misleading.

Benchmarked on A100 80GB with GPU fully free (training killed). Tested with pre-loaded data to eliminate DataLoader overhead.

| Config | FPS | Per-frame latency |
|--------|-----|-------------------|
| Single frame (batch=1) | **2.4 FPS** | 419ms avg, 305ms min, 500ms max |
| Batched (batch=8) | **2.4 FPS** | 412ms per frame |
| **VRAM usage** | | **8.83 GB peak** |

### Why so slow?

The paper's 130fps figure likely refers to **just the Gaussian rasterization step** (which is indeed very fast). But the full pipeline per frame includes:
1. Query triplane features for all 32,655 Gaussians
2. Transformer cross-attention (audio → spatial features)
3. Compute per-Gaussian deformation (position, scale, rotation, opacity, SH)
4. Gaussian rasterization → output image

Steps 1-3 (the deformation network) are the bottleneck, not step 4. Batching doesn't help because the deformation is per-frame (each frame has different audio).

### What this means

At 2.4 FPS, we need a **~12x speedup** to hit 30fps for real-time. Options to investigate:
- **torch.compile / TensorRT** — compile the deformation network for faster inference
- **FP16 / mixed precision** — halve the compute for deformation network
- **Reduce Gaussians** — fewer points = faster deformation (quality tradeoff)
- **Temporal interpolation** — run deformation every Nth frame, interpolate between
- **Smaller deformation network** — trade quality for speed
- **Different architecture** — some newer methods may have faster deformation

### Comparison with Tavus

Tavus runs at 40fps real-time. They likely have heavily optimized CUDA kernels and a custom inference pipeline. Our unoptimized Python + PyTorch pipeline has a lot of room for optimization before we know the true performance ceiling.

## Files

- Rendered frames: `~/Being/output/results/gaussiantalker_coarse/`
- Preview videos: `preview_coarse_7500.mp4`, `preview_fine_500.mp4`, `preview_fine_1500.mp4`, `preview_fine_2000.mp4`
- Text-to-video output: `~/Being/output/results/hello_roman.mp4`
- Training log: `/workspace/training_gt.log` (on A100 pod)
- Model output: `/workspace/Being/extern/GaussianTalker/output/roman_gt/` (on A100 pod)
- Scripts on pod: `/workspace/simple_render.py`, `/workspace/benchmark_fps.py`, `/workspace/text_to_video.py`

## Text-to-Video Pipeline — Working (with caveats)

Built an end-to-end script: `text_to_video.py` on the A100 pod at `/workspace/text_to_video.py`.

```bash
python3 /workspace/text_to_video.py --text "Hello, my name is Roman" --output /workspace/output.mp4
```

Pipeline: Text → Edge TTS → wav2vec features → GaussianTalker render → ffmpeg → mp4 with audio.

### Final working run

| Step | Time |
|------|------|
| TTS (Edge TTS) | ~2s |
| wav2vec feature extraction | ~8s (includes model load) |
| GaussianTalker render (137 frames) | ~142s (1.0 FPS) |
| ffmpeg assemble | ~1s |
| **Total** | **~155s for 5.5s video** |

Patches needed to get custom audio working:
- `talking_dataset_readers.py`: Changed custom camera source from `transforms_val.json` → `transforms_train.json`
- `talking_dataset_readers.py`: Added frame cycling/truncation to match audio feature count to camera count
- `text_to_video.py`: Audio features must be saved as `[N, 16, 44]` (Scene auto-permutes to `[N, 44, 16]`)

### Lip sync does NOT work

The avatar renders but **lips do not move with the audio at all**. Reasons:

1. **Only 10 seconds of training data** — The transformer audio decoder needs diverse audio→mouth shape pairings. With 250 frames of one phrase, it barely saw any mouth variation. It learned to reconstruct the face but NOT the audio→motion mapping.
2. **Dummy AU data** — We fed garbage for Action Units. Without real blink/expression signals from OpenFace, the expression conditioning pathway is dead.
3. **TTS vs natural speech distribution** — wav2vec features from synthetic Edge TTS voice have a different distribution than the natural speech in training. The model has never seen these patterns.
4. **Only 2000 fine iterations at 20%** — The audio→deformation mapping is the hardest thing to learn and needs both more data and more training time to converge.

**Bottom line**: Lip sync requires (a) much more training data with diverse speech, (b) real AU extraction, and (c) possibly more training iterations. The pipeline works, but the model needs better data to learn the audio→motion relationship.

Output: `~/Being/output/results/hello_roman.mp4`

---

## Proof of Concept: Conclusions

### What works
- Full pipeline: text → TTS → audio features → 3DGS render → video with synced audio
- Face rendering: solid, recognizable face from just 10s of video
- GaussianTalker trains in ~5 min (coarse) + fine stage converges fast
- InsTaG preprocessed data is ~95% reusable with GaussianTalker
- VRAM usage is reasonable (8.83 GB peak)

### What doesn't work yet
- **Lip sync** — needs more diverse training data, not a pipeline issue
- **Hair / top of head** — BFM mesh doesn't cover scalp, needs longer video with head movement
- **Mouth interior / teeth** — needs real teeth masks and more mouth-open training frames
- **Real-time speed** — 2.4 FPS (deformation network bottleneck), needs optimization
- **Expression / emotion** — no control yet, model is purely audio-driven with dead AU pathway

### What we proved
Even with 10 seconds of video (vs Tavus's 2 minutes), dummy AU data, and zero optimization, we got a working end-to-end pipeline. The architecture is sound. Quality is a data problem, speed is an optimization problem — both are solvable.

---

## Next Steps (for next session)

### Priority 1: Better data (biggest quality lever)
- [ ] Record 3-5 min video with diverse speech, head movement, expressions
- [ ] Build OpenFace for real AU extraction (AU45 blink + expression AUs)
- [ ] Set up real teeth/mouth segmentation (EasyPortrait or similar)
- [ ] Re-run full preprocessing pipeline with quality data

### Priority 2: Retrain with good data
- [ ] Train GaussianTalker with 3-5 min video + real AUs + real teeth masks
- [ ] Verify lip sync actually works with diverse speech data
- [ ] Compare quality vs 10s training run
- [ ] Run full fine stage (10000 iterations)

### Priority 3: Speed optimization
- [ ] Profile deformation network to find exact bottleneck
- [ ] Try `torch.compile()` on deformation network
- [ ] Try FP16 inference
- [ ] Investigate TensorRT compilation
- [ ] Benchmark after each optimization

### Priority 4: Production pipeline
- [ ] Wire up Being's `inference/engine.py` → GaussianTalker
- [ ] Streaming audio feature extraction (chunked, causal)
- [ ] WebSocket video streaming in `api/server.py`
- [ ] End-to-end latency benchmark

### Stretch: Diffusion refinement
- [ ] Test StreamDiffusion + SD-turbo on GaussianTalker output frames
- [ ] Tune denoise strength for quality vs identity preservation
- [ ] Benchmark combined pipeline FPS
