# 2026-02-22 — First GaussianTalker Training Run

## Summary

First end-to-end test of GaussianTalker on the roman dataset. Switched from InsTaG to GaussianTalker based on research (see RESEARCH.md). Goal: prove the pipeline works, not achieve final quality.

## Setup

- **Hardware**: A100 80GB on RunPod
- **Input data**: 10 seconds of video (250 frames at 25fps), reused from InsTaG preprocessing
- **Audio features**: wav2vec/esperanto 44-dim (copied as aud_ds.npy, changed audio_in_dim from 29→44)
- **Face model**: BFM 2009 (same as InsTaG), added vertices field to track_params.pt
- **Config**: `64_dim_1_transformer.py` — 64-dim model, 1 transformer layer, 32 batch size
- **AU data**: Dummy (no real OpenFace extraction yet)

## Data Compatibility

InsTaG preprocessed data was ~95% reusable. Only two fixes needed:
1. Added `vertices` key to `track_params.pt` (reconstructed from BFM coefficients)
2. Copied `aud_eo.npy` → `aud_ds.npy` and changed `audio_in_dim` from 29 to 44 in `scene/deformation.py`

## Installation Issues & Fixes

| Issue | Fix |
|-------|-----|
| Missing `open3d` | `pip install open3d` (needed `--ignore-installed blinker` first) |
| Missing `tkinter` | `apt-get install python3-tk` |
| Missing `cv2` | `pip install opencv-python-headless` |
| Missing `wandb` | `pip install wandb` + `WANDB_MODE=disabled` |
| `mmcv` too heavy / build timeout | Replaced with `mmengine` — patched `train.py` and `render.py` imports |
| `imageio` missing for render | `pip install imageio imageio-ffmpeg` |
| `ffmpeg` missing on pod | `apt-get install ffmpeg` |
| DataLoader crash (`num_workers=32`) | Reduced to `num_workers=4` in train.py (3 locations) |
| `render.py` hardcodes `process_until=1000` | Patched to `min(1000, len(viewpoint_stack.dataset))` for our 250-frame dataset |

## Training

### Coarse Stage (7999 iterations)
- **Time**: ~5 minutes
- **Speed**: ~28 it/s
- **Final PSNR**: ~34
- **Gaussians**: 32,655

### Fine Stage (10000 iterations, in progress)
- **Speed**: ~1.0-1.1 s/iter
- **Estimated total time**: ~3 hours
- **Currently at**: iter 1510 (15%), PSNR 34.67

Fine stage enables `lip_fine_tuning` and `depth_fine_tuning` for better mouth and depth quality.

## Render Quality Snapshots

### Coarse stage (iter 7500) — ghostly
- Face is transparent/ghostly, heavy artifacts
- General shape is there but not usable
- This is expected — coarse stage only learns rough geometry

### Fine stage (iter 500) — already solid
- Face becomes solid and recognizable
- Skin color and texture reasonable
- Massive jump from coarse → fine

### Fine stage (iter 1500) — improving
- Sharper features, more natural lighting
- Less waxy appearance
- Clear improvement over iter 500

### Known Issues (at iter 1500)
1. **Missing hair / top of head**: BFM mesh only covers the face (34,650 vertices), not scalp/hair. Hair must be learned via Gaussian densification from training images. With only 10s of tightly-cropped video, there's not enough signal for the hair region.
2. **No mouth interior**: BFM has no teeth/tongue geometry. Model must learn mouth interior purely from pixels. With limited mouth-open frames in 10s of video and dummy AU data, this is undertrained.

## Key Takeaway

With only **10 seconds** of input video and dummy AU data, GaussianTalker is already producing a recognizable, solid face render. For comparison, Tavus requires 2 minutes of video and has $30M+ in GPU infrastructure. The core pipeline works — quality improvements will come from:

1. **More data**: 3-5 min video → better hair, mouth, expression coverage
2. **Real AU extraction**: OpenFace → proper blink and expression conditioning
3. **Real teeth masks**: EasyPortrait or similar → mouth interior quality
4. **Full fine-stage training**: Currently only 15% done, quality still improving
5. **Diffusion refinement pass**: StreamDiffusion + SD-turbo for photorealism (Phase 2.5)

## Files

- Rendered frames: `~/Being/output/results/gaussiantalker_coarse/`
- Preview videos: `preview_coarse_7500.mp4`, `preview_fine_500.mp4`, `preview_fine_1500.mp4`
- Training log: `/workspace/training_gt.log` (on A100 pod)
- Model output: `/workspace/Being/extern/GaussianTalker/output/roman_gt/` (on A100 pod)
- Simple render script: `/workspace/simple_render.py` (on A100 pod)

## Next Steps

- [ ] Wait for fine stage to complete (10000 iterations, ~2.5h remaining)
- [ ] Render final output and compare quality progression
- [ ] Record longer video (3-5 min) for next training run
- [ ] Build OpenFace for real AU extraction
- [ ] Try real teeth segmentation masks
- [ ] Test diffusion refinement pass (StreamDiffusion + SD-turbo)
