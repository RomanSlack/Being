# 2026-02-23 — Training Day 2, Technology Landscape, and Strategic Thinking

## Training Update

### Setup
- **Hardware**: A100 80GB on RunPod (`ssh -i ~/.ssh/runpod_key -p 19870 root@154.54.102.23`)
- **Input data**: 6:43 video (10,081 frames at 25fps, 1080p) — full preprocessing pipeline completed overnight
- **Config**: `64_dim_1_transformer.py`, fine stage 10,000 iterations
- **Pod**: `/workspace/Being/extern/GaussianTalker/`, output at `output/roman_v2/`

### Training Progress (fine stage)
| Checkpoint | PSNR | Loss | Notes |
|-----------|------|------|-------|
| iter 500 | ~33.5 | ~0.040 | Early, recognizable face |
| iter 1500 | ~34.5 | ~0.035 | First audio-driven video rendered from here |
| iter 3000 | ~35.1 | ~0.035 | Steady improvement |
| iter 4000 | ~35.6 | ~0.033 | |
| iter 5000 | ~36.0 | ~0.032 | |
| iter 6000 | ~36.5 | ~0.031 | Second audio-driven video — significant quality jump |
| iter 6500 | ~36.3 | ~0.031 | Training still running, ETA ~1.5h to finish |

Training speed: ~2.2s/iter when GPU is uncontested. Slows to ~5s/iter if render processes are competing for GPU. Always kill stale render processes before checking speed.

### Key Scripts on Pod
- `/workspace/audio_to_video.py` — full audio-to-video pipeline (wav2vec → GaussianTalker → ffmpeg)
- `/workspace/render_preview.py` — render preview from training frames at any checkpoint
- Change `ITERATION=N` in either script to render from a different checkpoint

### Audio-to-Video Pipeline
Working end-to-end: provide any audio → wav2vec feature extraction → GaussianTalker render → synced mp4.
- Audio features: `cpierse/wav2vec2-large-xlsr-53-esperanto`, 44-dim, windowed to `[N, 16, 44]`
- Manual sliding window (not torch unfold — that produced wrong shapes)
- Pass custom audio to `Scene()` constructor via `custom_aud=path` kwarg
- Render via `DataLoader` with `collate_fn=list` (Camera objects can't be default-collated)
- 474 frames (19s) renders in ~12.5 minutes on A100

### Local Outputs
All at `/home/roman/Being/output/results/roman_v2/`:
- `test_audio_driven_iter1500.mp4` — first audio-driven render
- `test_audio_driven_iter1500_gazefixed.mp4` — gaze corrected (strength 0.7)
- `test_audio_driven_iter1500_gaze1.0.mp4` — gaze corrected (strength 1.0)
- `test_audio_driven_iter6000.mp4` — much better quality render
- `test_audio_driven_iter6000_gazefixed.mp4` — gaze corrected iter 6000

### Local Backups
All at `/home/roman/Being/data/roman_v2/`:
- `track_params.pt` (4.0GB) — 12 hours of face tracking, most critical artifact
- `roman_preprocess_backup.tar.gz` (3.6GB) — full preprocessing data
- `model_checkpoints/model_checkpoint_fine500.tar.gz` (54MB) — early checkpoint
- Various audio files, transforms, AU data

---

## Quality Assessment

### iter 6000 Results
The iter 6000 audio-driven video looks **significantly better** than iter 1500. Face quality is high, skin looks natural, and the overall rendering is impressive. Roman described it as "absolutely amazing."

### Remaining Issues
1. **Static upper face** — No eyebrow movement, eyes don't track, forehead is frozen. This is an **architectural limitation of GaussianTalker**, not a training issue. The deformation network only accepts audio + AU45 (blink) + camera params. There's no input pathway for eyebrow, eye gaze, or upper face expressions.
2. **Limited lip movement** — Lips move but not as much as they should. This may improve with more training iterations (currently only 65% done).
3. **Static eye gaze** — Eyes stare straight ahead regardless of audio. Gaze correction via MediaPipe iris warping helps slightly but is "barely noticeable" even at full strength.

### Gaze Correction Script
Built `/home/roman/Being/scripts/fix_gaze.py`:
- Uses MediaPipe Face Landmarker (new tasks API) + OpenCV radial warping
- Model file at `/home/roman/Being/models/face_landmarker.task`
- Runs at ~50fps on RTX 4070 Super locally
- Conda env: `being` (Python 3.10, mediapipe + opencv)
- Effect is subtle — iris warping has limited impact when rendered eyes are already near-center

---

## Technology Landscape — Strategic Thoughts

### Where We Are
We're building a **talking head avatar** — record yourself → get a photorealistic clone driven by any audio. This is the same category as **Tavus.io** ($28M raised, 2-min recording → real-time avatar). Our approach: 3D Gaussian Splatting via GaussianTalker.

### The Competitive Landscape (Feb 2026)

**Category 1: Per-person 3DGS/NeRF models (our approach)**
- Tavus.io, Being (us), GaussianTalker, SyncTalk, FlashAvatar
- Pros: Persistent 3D identity, consistent rendering, small model per person, fast inference once trained
- Cons: Requires training per person, expression control is model-dependent, can look "frozen" if architecture doesn't support enough expression dimensions

**Category 2: Real-time diffusion (Decart/Lucy)**
- Decart AI ($3.1B valuation), Lucy 2.0, MirageLSD
- What it does: Real-time image-to-image diffusion at 30fps, 1080p. Transforms webcam feed into any character/style with <40ms latency
- Pros: Incredibly flexible, no per-person training, any style/character
- Cons: **Requires a webcam/video input** — it's transformation, not generation. Can't create speech from thin air. No persistent 3D model. Identity consistency can drift.

**Category 3: 2D talking head models**
- SadTalker, Wav2Lip, LivePortrait, Hallo2
- Pros: Simple, fast, some work out of the box
- Cons: 2D warping artifacts, uncanny valley, poor with large head movements, no real 3D understanding

**Category 4: FLAME-driven 3DGS (next gen)**
- FlashAvatar (300+ fps), GaussianAvatars, SplattingAvatar
- Pros: Full FLAME expression control (100 dims — jaw, lips, eyes, brows, cheeks), trains in minutes
- Cons: Newer, less battle-tested

### Why Decart's Approach Is Different From Ours

Decart's Lucy model is a **video-to-video diffusion transformer** — it takes an input video stream (webcam) and transforms it in real-time. Think of it as a hyper-fast Img2Img pipeline. The "character mapping" requires **someone performing in real-time** — it's a filter, not an autonomous avatar.

Our goal with Being is fundamentally different: we want an avatar that speaks **without any live video input**. You provide audio (or text → TTS), and the avatar generates speech from scratch. This is what Tavus does. You can't do this with Decart's approach because there's no source video to transform.

Decart's $3.1B valuation makes sense when you realize they're building **real-time VFX infrastructure** — the same tech powers gaming, streaming, AR, virtual production, not just face swapping. Their TAM is way bigger than talking heads.

### Are We On The Right Track?

**Yes.** The 3DGS per-person model approach is the right one for autonomous talking head avatars. Here's why:

1. **It's what the market leaders use.** Tavus, HeyGen, and Synthesia all use per-person trained models (varying architectures). Tavus specifically uses NeRF/3DGS.

2. **3D > 2D for this use case.** 2D methods (SadTalker, Wav2Lip) warp pixels and break down with head movement. 3D methods render from a true 3D representation and handle arbitrary viewpoints.

3. **Our results after 1 day are promising.** Iter 6000 already looks "amazing" with a 6:43 training video. Tavus requires 2 minutes. We're in the same ballpark.

4. **The path to improvement is clear:**
   - **More expression control** → Switch to FLAME-driven model (FlashAvatar). This is the #1 priority after current training finishes. See `RESEARCH_flame_avatars.md` for the detailed plan.
   - **Better lip sync** → DiffPoseTalk (audio → FLAME params, SIGGRAPH 2024) gives much richer audio-to-motion mapping than GaussianTalker's simple transformer
   - **Real-time speed** → FlashAvatar renders at 300+ fps vs GaussianTalker's 2.4fps. The speed problem may solve itself with better architecture.
   - **Diffusion refinement layer** → Run a fast diffusion model (SDXL-Turbo, LCM) on rendered frames to fix artifacts, add detail, improve realism. StreamDiffusion can do this at 100+ fps.

### Recommended Next Steps (Priority Order)

1. **Finish current GaussianTalker training** (iter 10000) — render final video, compare iter 6000 vs 10000
2. **Try FlashAvatar** — it solves the frozen upper face problem architecturally (FLAME has eyebrow, gaze, jaw, cheek params). See `RESEARCH_flame_avatars.md`.
3. **Add DiffPoseTalk** for audio → FLAME expression mapping
4. **Benchmark FlashAvatar speed** — if 300+ fps holds, real-time is immediately viable
5. **Wire up Being's server** (`being/api/server.py`, `being/inference/engine.py`) once we have a fast, expressive model
6. **Add diffusion refinement** as a polish layer

### The Long-Term Stack

```
Audio Input
    ↓
DiffPoseTalk (audio → FLAME expression params)
    ↓
FlashAvatar (FLAME → 3DGS render, 300+ fps)
    ↓
Gaze correction (MediaPipe or direct FLAME eye control)
    ↓
Optional: Fast diffusion refinement (StreamDiffusion, 1 step)
    ↓
NVENC H.264 encode → WebRTC stream
```

Total latency budget: ~100-150ms, well within real-time.

---

## Reference: Key Files

| File | Purpose |
|------|---------|
| `RESEARCH.md` | Original GaussianTalker vs InsTaG research |
| `RESEARCH_flame_avatars.md` | FlashAvatar + DiffPoseTalk deep dive |
| `PLAN_gaussiantalker.md` | Original GaussianTalker setup plan |
| `notes/2026-02-22.md` | Day 1 — first training run, 10s video, proof of concept |
| `notes/2026-02-23.md` | This file — day 2, 6:43 video training, technology strategy |
| `notes/2026-02-23-gaze-redirection-research.md` | Gaze correction options research |
| `scripts/fix_gaze.py` | MediaPipe gaze correction post-processor |
| `being/api/server.py` | FastAPI + WebSocket server (scaffolded, needs model wiring) |
| `being/inference/engine.py` | Real-time inference engine (scaffolded, needs model wiring) |
